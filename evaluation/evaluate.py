"""
evaluate.py
Phase 6: Evaluation Pipeline for OV-RAG vs Plain RAG

Runs 5 questions across 100 contracts under two conditions (OV-RAG with
ontology validation, Plain RAG without) and computes precision, recall,
F1, correction success rate, hard-reject rate, per-clash-type breakdown,
NLP quality metrics (ROUGE-L, BERTScore), and latency measurements.

Ground truth (including reference answers) is loaded from
contract_ground_truth.json (generated by generate_test_pdfs.py).

Usage:
  python evaluate.py                          # Full evaluation (1000 queries)
  python evaluate.py --dry-run                # Print plan, no API calls
  python evaluate.py --contracts 001 061      # Subset of contracts
  python evaluate.py --questions Q1 Q3        # Subset of questions
  python evaluate.py --conditions ovrag       # Single condition
  python evaluate.py --resume                 # Skip already-completed queries
"""

import argparse
import contextlib
import csv
import io
import json
import os
import sys
import time
import traceback
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional

from dotenv import load_dotenv
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception,
)

load_dotenv()

# ---------------------------------------------------------------------------
# Ground truth — loaded from JSON (generated by generate_test_pdfs.py)
# ---------------------------------------------------------------------------
GROUND_TRUTH_PATH = Path("contract_ground_truth.json")


def _load_ground_truth() -> Dict[str, Dict]:
    """Load ground truth from JSON file."""
    if not GROUND_TRUTH_PATH.exists():
        print(f"[X] Ground truth file not found: {GROUND_TRUTH_PATH}")
        print("    Run 'python generate_test_pdfs.py' first to generate it.")
        sys.exit(1)

    with open(GROUND_TRUTH_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


GROUND_TRUTH = _load_ground_truth()

# ---------------------------------------------------------------------------
# Questions
# ---------------------------------------------------------------------------
QUESTIONS: List[Dict] = [
    {
        "id": "Q1",
        "text": (
            "What type of loan is described in this document? Is it a consumer "
            "loan, commercial loan, mortgage, student loan, or another type?"
        ),
        "target": "Loan type classification",
    },
    {
        "id": "Q2",
        "text": (
            "Who is the borrower and who is the lender of this loan? "
            "Are they individuals or organizations?"
        ),
        "target": "Party classification",
    },
    {
        "id": "Q3",
        "text": (
            "Is this loan secured or unsecured? If secured, what collateral "
            "is specified?"
        ),
        "target": "SecuredLoan / UnsecuredLoan disjointness",
    },
    {
        "id": "Q4",
        "text": (
            "Is this a revolving (open-end) credit facility or a fixed-term "
            "(closed-end) loan? What are the repayment terms?"
        ),
        "target": "OpenEndCredit / ClosedEndCredit disjointness",
    },
    {
        "id": "Q5",
        "text": (
            "Summarize the key financial terms: principal amount, interest "
            "rate, loan type, parties involved, and whether the loan is secured."
        ),
        "target": "Comprehensive extraction",
    },
]

QUESTION_MAP = {q["id"]: q for q in QUESTIONS}

# ---------------------------------------------------------------------------
# Rate-limit retry predicate
# ---------------------------------------------------------------------------
def _is_rate_limit_error(exc: BaseException) -> bool:
    """Return True if the exception is an OpenAI 429 rate-limit error."""
    try:
        import openai
        if isinstance(exc, openai.RateLimitError):
            return True
    except ImportError:
        pass
    return "429" in str(exc)

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------
SEPARATOR = "=" * 70


def _contract_pdf(contract_id: str) -> str:
    return f"data/Contract_{contract_id}.pdf"


def _serialise_result(result: dict) -> dict:
    """Make a process_query result JSON-serialisable."""
    out = {}
    for k, v in result.items():
        if k == "sources":
            out["num_sources"] = len(v) if v else 0
            continue
        if k == "validation":
            if v is not None:
                out["validation_is_valid"] = v.is_valid
                out["validation_explanation"] = v.explanation
            else:
                out["validation_is_valid"] = None
                out["validation_explanation"] = None
            continue
        out[k] = v
    return out


# ---------------------------------------------------------------------------
# EvaluationRunner
# ---------------------------------------------------------------------------
class EvaluationRunner:
    """Runs the OV-RAG evaluation across contracts, questions, and conditions."""

    def __init__(
        self,
        contracts: List[str],
        questions: List[str],
        conditions: List[str],
        output_dir: str = "evaluation_output",
        resume: bool = False,
        dry_run: bool = False,
    ):
        self.contracts = contracts
        self.question_ids = questions
        self.conditions = conditions
        self.output_dir = Path(output_dir)
        self.resume = resume
        self.dry_run = dry_run

        self.results: List[dict] = []
        self.existing_keys: set = set()

        # JSONL path for incremental saves
        self.jsonl_path = self.output_dir / "evaluation_results.jsonl"

        # Progress callback (used by Streamlit Batch page)
        self.on_progress = None  # callable(completed, total, row)

        # Lazy-loaded NLP scorers
        self._rouge_scorer = None

    # ------------------------------------------------------------------
    # NLP scoring helpers
    # ------------------------------------------------------------------
    def _get_rouge_scorer(self):
        if self._rouge_scorer is None:
            from rouge_score import rouge_scorer
            self._rouge_scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
        return self._rouge_scorer

    def _compute_nlp_scores(self, answer: str, reference: str) -> dict:
        """Compute ROUGE-L and BERTScore between answer and reference."""
        null_scores = {
            "rouge_l": None,
            "bertscore_precision": None,
            "bertscore_recall": None,
            "bertscore_f1": None,
        }
        if not answer or not reference:
            return null_scores

        try:
            # ROUGE-L
            scorer = self._get_rouge_scorer()
            rouge_scores = scorer.score(reference, answer)
            rouge_l = rouge_scores["rougeL"].fmeasure

            # BERTScore
            from bert_score import score as bert_score_fn
            P, R, F1 = bert_score_fn(
                [answer], [reference], lang="en", verbose=False
            )

            return {
                "rouge_l": round(rouge_l, 4),
                "bertscore_precision": round(P[0].item(), 4),
                "bertscore_recall": round(R[0].item(), 4),
                "bertscore_f1": round(F1[0].item(), 4),
            }
        except Exception as e:
            print(f"  [!] NLP scoring failed: {e}")
            return null_scores

    # ------------------------------------------------------------------
    # Public
    # ------------------------------------------------------------------
    def run(self):
        """Main entry point — run the full evaluation."""
        self.output_dir.mkdir(parents=True, exist_ok=True)

        if self.resume:
            self._load_existing_results()

        plan = self._build_plan()

        if self.dry_run:
            self._print_plan(plan)
            return

        print(f"\n{SEPARATOR}")
        print("OV-RAG EVALUATION PIPELINE")
        print(SEPARATOR)
        print(f"Contracts : {len(self.contracts)} ({self.contracts[0]}..{self.contracts[-1]})")
        print(f"Questions : {', '.join(self.question_ids)}")
        print(f"Conditions: {', '.join(self.conditions)}")
        print(f"Total queries: {len(plan)}")
        if self.existing_keys:
            print(f"Resuming — {len(self.existing_keys)} already completed, skipping")
        print(SEPARATOR)

        # Shared components (initialised once)
        from validator import OntologyValidator
        from extractor import TripleExtractor

        validator = OntologyValidator()
        extractor = TripleExtractor()

        completed = 0
        total = len(plan)
        start_time = time.time()

        for entry in plan:
            contract_id = entry["contract_id"]
            condition = entry["condition"]
            q = entry["question"]
            key = (contract_id, condition, q["id"])

            if key in self.existing_keys:
                completed += 1
                continue

            completed += 1
            print(f"\n{'—' * 70}")
            print(
                f"[{completed}/{total}] Contract {contract_id} | "
                f"{condition.upper()} | {q['id']}"
            )
            print(f"  Q: {q['text'][:80]}...")
            print(f"{'—' * 70}")

            row = self._run_single_query(
                contract_id, condition, q, validator, extractor
            )
            self.results.append(row)
            self._save_incremental(row)

            if self.on_progress:
                self.on_progress(completed, total, row)

            # Rate-limit cooldown between API calls
            time.sleep(2)

        elapsed = time.time() - start_time

        # Reload all results (including resumed) for final metrics
        all_results = self._load_all_jsonl()

        metrics = self._compute_metrics(all_results)
        clash_metrics = self._compute_per_clash_type_metrics(all_results)
        ab_comparison = self._compute_ab_comparison(all_results, metrics)
        self._print_summary(all_results, metrics, clash_metrics, elapsed)
        self._save_outputs(all_results, metrics, clash_metrics, ab_comparison)

    # ------------------------------------------------------------------
    # Single query execution (with retry for rate limits)
    # ------------------------------------------------------------------
    def _run_single_query(
        self,
        contract_id: str,
        condition: str,
        question: dict,
        validator,
        extractor,
    ) -> dict:
        """Run one (contract, condition, question) combination."""
        from main import OVRAGSystem
        from rag_pipeline import RAGPipeline

        gt = GROUND_TRUTH[contract_id]
        row = {
            "contract_id": contract_id,
            "condition": condition,
            "question_id": question["id"],
            "question_text": question["text"],
            "question_target": question["target"],
            "ground_truth": gt["label"],
            "expect_clash": gt["expect_clash"],
            "clash_type": gt.get("clash_type"),
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
        }

        # Reference answer for NLP metrics
        ref_answers = gt.get("reference_answers", {})
        reference_answer = ref_answers.get(question["id"], "")

        try:
            row = self._run_query_with_retry(
                row, contract_id, condition, question, validator, extractor,
                reference_answer, OVRAGSystem, RAGPipeline,
            )
        except Exception as e:
            row["error"] = f"{type(e).__name__}: {str(e)}"
            row["answer"] = None
            row["validation_passed"] = None
            row["pipeline_output"] = traceback.format_exc()[-2000:]
            row["latency_total"] = None
            row.update({
                "rouge_l": None, "bertscore_precision": None,
                "bertscore_recall": None, "bertscore_f1": None,
            })

        return row

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=2, min=4, max=60),
        retry=retry_if_exception(_is_rate_limit_error),
        reraise=True,
    )
    def _run_query_with_retry(
        self, row, contract_id, condition, question, validator, extractor,
        reference_answer, OVRAGSystem, RAGPipeline,
    ) -> dict:
        """Inner query execution wrapped with tenacity retry for 429 errors."""
        # Build a fresh system, sharing validator + extractor
        system = OVRAGSystem.__new__(OVRAGSystem)
        system.api_key = os.getenv("OPENAI_API_KEY")
        system.rag = RAGPipeline(api_key=system.api_key)
        system.extractor = extractor
        system.validator = validator

        # Load only this contract
        pdf_path = _contract_pdf(contract_id)
        if not Path(pdf_path).exists():
            row["error"] = f"PDF not found: {pdf_path}"
            row["answer"] = None
            row["validation_passed"] = None
            return row

        # Suppress verbose pipeline stdout
        captured = io.StringIO()
        with contextlib.redirect_stdout(captured):
            system.load_documents([pdf_path])
            validate = condition == "ovrag"
            result = system.process_query(question["text"], validate=validate)

        # Extract fields
        serialised = _serialise_result(result)
        row.update(serialised)

        # Determine validation outcome for metrics
        if not validate:
            row["validation_passed"] = None  # not applicable
        elif result.get("validation") is not None:
            row["validation_passed"] = result["validation"].is_valid
        elif not result.get("triples"):
            row["validation_passed"] = None  # 0 triples — exclude
        else:
            row["validation_passed"] = None

        row["error"] = None
        row["pipeline_output"] = captured.getvalue()[-2000:]  # last 2k chars

        # Latency (from process_query timing)
        row["latency_rag"] = result.get("latency_rag")
        row["latency_extraction"] = result.get("latency_extraction")
        row["latency_validation"] = result.get("latency_validation")
        row["latency_total"] = result.get("latency_total")

        # NLP metrics (ROUGE-L + BERTScore)
        answer_text = row.get("answer") or ""
        nlp_scores = self._compute_nlp_scores(answer_text, reference_answer)
        row.update(nlp_scores)

        return row

    # ------------------------------------------------------------------
    # Plan
    # ------------------------------------------------------------------
    def _build_plan(self) -> List[dict]:
        """Build the list of (contract, condition, question) to run."""
        plan = []
        for contract_id in self.contracts:
            for condition in self.conditions:
                for qid in self.question_ids:
                    q = QUESTION_MAP[qid]
                    plan.append(
                        {
                            "contract_id": contract_id,
                            "condition": condition,
                            "question": q,
                        }
                    )
        return plan

    def _print_plan(self, plan: List[dict]):
        """Print the evaluation plan (dry-run mode)."""
        print(f"\n{SEPARATOR}")
        print("DRY RUN — Evaluation Plan")
        print(SEPARATOR)
        print(f"Total queries: {len(plan)}")
        print()
        for i, entry in enumerate(plan, 1):
            c = entry["contract_id"]
            cond = entry["condition"].upper()
            q = entry["question"]
            gt = GROUND_TRUTH[c]["label"]
            clash = GROUND_TRUTH[c].get("clash_type") or ""
            label = f"{gt}" + (f" ({clash})" if clash else "")
            print(f"  {i:4d}. Contract {c} ({label:<25s}) | {cond:5s} | {q['id']} — {q['target']}")
        print()
        print("No API calls made. Remove --dry-run to execute.")

    # ------------------------------------------------------------------
    # Incremental save / resume
    # ------------------------------------------------------------------
    def _save_incremental(self, row: dict):
        """Append one result as a JSON line."""
        save_row = {k: v for k, v in row.items() if k != "pipeline_output"}
        with open(self.jsonl_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(save_row, ensure_ascii=False) + "\n")

    def _load_existing_results(self):
        """Load already-completed results for --resume."""
        if not self.jsonl_path.exists():
            return
        with open(self.jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    row = json.loads(line)
                    key = (row["contract_id"], row["condition"], row["question_id"])
                    self.existing_keys.add(key)
                except (json.JSONDecodeError, KeyError):
                    continue
        print(f"[OK] Loaded {len(self.existing_keys)} existing result(s) for resume")

    def _load_all_jsonl(self) -> List[dict]:
        """Load all results from JSONL."""
        results = []
        if not self.jsonl_path.exists():
            return results
        with open(self.jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    results.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
        return results

    # ------------------------------------------------------------------
    # Metrics
    # ------------------------------------------------------------------
    def _compute_metrics(self, results: List[dict]) -> dict:
        """Compute confusion matrix, NLP metrics, and latency for both conditions."""
        ovrag_results = [
            r for r in results
            if r.get("condition") == "ovrag" and r.get("validation_passed") is not None
        ]

        tp = fp = tn = fn = 0
        corrections_needed = 0
        corrections_succeeded = 0
        hard_rejects = 0

        for r in ovrag_results:
            expect_clash = r.get("expect_clash", False)
            validation_passed = r["validation_passed"]

            if expect_clash:
                if not validation_passed:
                    tp += 1
                else:
                    fn += 1
            else:
                if validation_passed:
                    tn += 1
                else:
                    fp += 1

            total_attempts = r.get("total_attempts", 1)
            if total_attempts > 1:
                corrections_needed += 1
                if r.get("hard_reject", False):
                    hard_rejects += 1
                else:
                    corrections_succeeded += 1

        precision = tp / (tp + fp) if (tp + fp) > 0 else None
        recall = tp / (tp + fn) if (tp + fn) > 0 else None
        f1 = (
            2 * precision * recall / (precision + recall)
            if precision is not None and recall is not None and (precision + recall) > 0
            else None
        )

        total_ovrag = len(ovrag_results)
        correction_success_rate = (
            corrections_succeeded / corrections_needed
            if corrections_needed > 0
            else None
        )
        hard_reject_rate = (
            hard_rejects / total_ovrag if total_ovrag > 0 else None
        )

        # --- NLP Metrics (averages per condition) ---
        def _avg_field(rows, field):
            vals = [r[field] for r in rows if r.get(field) is not None]
            return round(sum(vals) / len(vals), 4) if vals else None

        ovrag_all = [r for r in results if r.get("condition") == "ovrag"]
        plain_all = [r for r in results if r.get("condition") == "plain"]

        # --- Latency (averages per condition) ---
        ovrag_latencies = [r["latency_total"] for r in ovrag_all if r.get("latency_total") is not None]
        plain_latencies = [r["latency_total"] for r in plain_all if r.get("latency_total") is not None]
        avg_latency_ovrag = round(sum(ovrag_latencies) / len(ovrag_latencies), 2) if ovrag_latencies else None
        avg_latency_plain = round(sum(plain_latencies) / len(plain_latencies), 2) if plain_latencies else None

        latency_overhead_seconds = None
        latency_overhead_percent = None
        if avg_latency_ovrag is not None and avg_latency_plain is not None and avg_latency_plain > 0:
            latency_overhead_seconds = round(avg_latency_ovrag - avg_latency_plain, 2)
            latency_overhead_percent = round(
                (avg_latency_ovrag - avg_latency_plain) / avg_latency_plain * 100, 1
            )

        return {
            "tp": tp,
            "fp": fp,
            "tn": tn,
            "fn": fn,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "total_ovrag_queries": total_ovrag,
            "total_plain_queries": len(plain_all),
            "corrections_needed": corrections_needed,
            "corrections_succeeded": corrections_succeeded,
            "hard_rejects": hard_rejects,
            "correction_success_rate": correction_success_rate,
            "hard_reject_rate": hard_reject_rate,
            # NLP metrics
            "avg_rouge_l_ovrag": _avg_field(ovrag_all, "rouge_l"),
            "avg_rouge_l_plain": _avg_field(plain_all, "rouge_l"),
            "avg_bertscore_f1_ovrag": _avg_field(ovrag_all, "bertscore_f1"),
            "avg_bertscore_f1_plain": _avg_field(plain_all, "bertscore_f1"),
            "avg_bertscore_precision_ovrag": _avg_field(ovrag_all, "bertscore_precision"),
            "avg_bertscore_precision_plain": _avg_field(plain_all, "bertscore_precision"),
            "avg_bertscore_recall_ovrag": _avg_field(ovrag_all, "bertscore_recall"),
            "avg_bertscore_recall_plain": _avg_field(plain_all, "bertscore_recall"),
            # Latency
            "avg_latency_ovrag": avg_latency_ovrag,
            "avg_latency_plain": avg_latency_plain,
            "latency_overhead_seconds": latency_overhead_seconds,
            "latency_overhead_percent": latency_overhead_percent,
        }

    def _compute_per_clash_type_metrics(self, results: List[dict]) -> dict:
        """Compute detection metrics broken down by clash type."""
        ovrag_results = [
            r for r in results
            if r.get("condition") == "ovrag" and r.get("validation_passed") is not None
        ]

        # Group by clash_type (None = clean)
        by_type = defaultdict(lambda: {"tp": 0, "fp": 0, "tn": 0, "fn": 0, "total": 0})

        for r in ovrag_results:
            clash_type = r.get("clash_type") or "clean"
            expect_clash = r.get("expect_clash", False)
            validation_passed = r["validation_passed"]
            by_type[clash_type]["total"] += 1

            if expect_clash:
                if not validation_passed:
                    by_type[clash_type]["tp"] += 1
                else:
                    by_type[clash_type]["fn"] += 1
            else:
                if validation_passed:
                    by_type[clash_type]["tn"] += 1
                else:
                    by_type[clash_type]["fp"] += 1

        # Compute derived metrics per type
        for ctype, m in by_type.items():
            tp, fp, fn = m["tp"], m["fp"], m["fn"]
            m["precision"] = tp / (tp + fp) if (tp + fp) > 0 else None
            m["recall"] = tp / (tp + fn) if (tp + fn) > 0 else None
            p, r = m["precision"], m["recall"]
            m["f1"] = (
                2 * p * r / (p + r)
                if p is not None and r is not None and (p + r) > 0
                else None
            )
            m["detection_rate"] = tp / (tp + fn) if (tp + fn) > 0 else None

        return dict(by_type)

    def _compute_ab_comparison(self, results: List[dict], metrics: dict) -> List[dict]:
        """Compute A/B comparison table: OV-RAG vs Plain RAG."""
        _fmt = lambda v: f"{v:.4f}" if v is not None else "N/A"
        _fmts = lambda v: f"{v:.2f}s" if v is not None else "N/A"

        # Clash detection rate (OV-RAG only)
        tp = metrics.get("tp", 0)
        fn = metrics.get("fn", 0)
        clash_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else None

        rows = [
            {
                "metric": "Avg ROUGE-L",
                "plain_rag": _fmt(metrics.get("avg_rouge_l_plain")),
                "ovrag": _fmt(metrics.get("avg_rouge_l_ovrag")),
                "difference": _fmt(
                    (metrics.get("avg_rouge_l_ovrag") or 0) - (metrics.get("avg_rouge_l_plain") or 0)
                ) if metrics.get("avg_rouge_l_ovrag") is not None and metrics.get("avg_rouge_l_plain") is not None else "N/A",
            },
            {
                "metric": "Avg BERTScore-F1",
                "plain_rag": _fmt(metrics.get("avg_bertscore_f1_plain")),
                "ovrag": _fmt(metrics.get("avg_bertscore_f1_ovrag")),
                "difference": _fmt(
                    (metrics.get("avg_bertscore_f1_ovrag") or 0) - (metrics.get("avg_bertscore_f1_plain") or 0)
                ) if metrics.get("avg_bertscore_f1_ovrag") is not None and metrics.get("avg_bertscore_f1_plain") is not None else "N/A",
            },
            {
                "metric": "Avg Latency",
                "plain_rag": _fmts(metrics.get("avg_latency_plain")),
                "ovrag": _fmts(metrics.get("avg_latency_ovrag")),
                "difference": f"+{metrics.get('latency_overhead_seconds', 0):.2f}s ({metrics.get('latency_overhead_percent', 0):.1f}%)"
                if metrics.get("latency_overhead_seconds") is not None else "N/A",
            },
            {
                "metric": "Clash Detection Rate",
                "plain_rag": "N/A (no validator)",
                "ovrag": _fmt(clash_detection_rate),
                "difference": "—",
            },
            {
                "metric": "Hard-Reject Rate",
                "plain_rag": "N/A",
                "ovrag": _fmt(metrics.get("hard_reject_rate")),
                "difference": "—",
            },
        ]
        return rows

    # ------------------------------------------------------------------
    # Output
    # ------------------------------------------------------------------
    def _print_summary(
        self, results: List[dict], metrics: dict, clash_metrics: dict, elapsed: float
    ):
        """Print a summary table to the console."""
        print(f"\n{SEPARATOR}")
        print("EVALUATION RESULTS")
        print(SEPARATOR)

        # Per-contract summary
        print("\nPer-Contract Results (OV-RAG):")
        print(f"  {'Contract':<12} {'GT':<10} {'Clash Type':<22} {'Passed':<8} {'Failed':<8} {'N/A':<6}")
        print(f"  {'—' * 66}")
        for cid in sorted(set(r["contract_id"] for r in results)):
            ovrag_rows = [
                r for r in results
                if r["contract_id"] == cid and r.get("condition") == "ovrag"
            ]
            gt = GROUND_TRUTH.get(cid, {})
            label = gt.get("label", "?")
            ctype = gt.get("clash_type") or ""
            passed = sum(1 for r in ovrag_rows if r.get("validation_passed") is True)
            failed = sum(1 for r in ovrag_rows if r.get("validation_passed") is False)
            none_ = sum(1 for r in ovrag_rows if r.get("validation_passed") is None)
            print(f"  {cid:<12} {label:<10} {ctype:<22} {passed:<8} {failed:<8} {none_:<6}")

        # Confusion matrix
        print(f"\nConfusion Matrix (OV-RAG):")
        print(f"  TP (clash detected)    : {metrics['tp']}")
        print(f"  FP (false alarm)       : {metrics['fp']}")
        print(f"  TN (correctly passed)  : {metrics['tn']}")
        print(f"  FN (missed clash)      : {metrics['fn']}")

        # Derived metrics
        _fmt = lambda v: f"{v:.4f}" if v is not None else "N/A"
        print(f"\nMetrics:")
        print(f"  Precision              : {_fmt(metrics['precision'])}")
        print(f"  Recall                 : {_fmt(metrics['recall'])}")
        print(f"  F1                     : {_fmt(metrics['f1'])}")
        print(f"  Correction success rate: {_fmt(metrics['correction_success_rate'])}")
        print(f"  Hard-reject rate       : {_fmt(metrics['hard_reject_rate'])}")

        # NLP Metrics
        print(f"\nNLP Quality Metrics:")
        print(f"  Avg ROUGE-L (OV-RAG)   : {_fmt(metrics.get('avg_rouge_l_ovrag'))}")
        print(f"  Avg ROUGE-L (Plain)    : {_fmt(metrics.get('avg_rouge_l_plain'))}")
        print(f"  Avg BERTScore-F1 (OV-RAG): {_fmt(metrics.get('avg_bertscore_f1_ovrag'))}")
        print(f"  Avg BERTScore-F1 (Plain): {_fmt(metrics.get('avg_bertscore_f1_plain'))}")

        # Latency
        print(f"\nLatency:")
        _fmts = lambda v: f"{v:.2f}s" if v is not None else "N/A"
        print(f"  Avg Latency (OV-RAG)   : {_fmts(metrics.get('avg_latency_ovrag'))}")
        print(f"  Avg Latency (Plain)    : {_fmts(metrics.get('avg_latency_plain'))}")
        print(f"  Overhead               : {_fmts(metrics.get('latency_overhead_seconds'))} "
              f"({metrics.get('latency_overhead_percent', 'N/A')}%)")

        # Per clash-type breakdown
        print(f"\nPer Clash-Type Detection Rates:")
        print(f"  {'Type':<22} {'TP':<6} {'FP':<6} {'TN':<6} {'FN':<6} {'Det.Rate':<10} {'F1':<10}")
        print(f"  {'—' * 66}")
        for ctype in sorted(clash_metrics.keys()):
            m = clash_metrics[ctype]
            dr = _fmt(m["detection_rate"])
            f1 = _fmt(m["f1"])
            print(
                f"  {ctype:<22} {m['tp']:<6} {m['fp']:<6} {m['tn']:<6} {m['fn']:<6} "
                f"{dr:<10} {f1:<10}"
            )

        # Totals
        print(f"\nTotals:")
        print(f"  OV-RAG queries         : {metrics['total_ovrag_queries']}")
        print(f"  Plain RAG queries      : {metrics['total_plain_queries']}")
        print(f"  Corrections needed     : {metrics['corrections_needed']}")
        print(f"  Corrections succeeded  : {metrics['corrections_succeeded']}")
        print(f"  Hard rejects           : {metrics['hard_rejects']}")
        print(f"  Elapsed time           : {elapsed:.1f}s")

        print(SEPARATOR)

    def _save_outputs(
        self, results: List[dict], metrics: dict,
        clash_metrics: dict, ab_comparison: List[dict],
    ):
        """Save final JSON and CSV files."""
        # 1. Combined JSON
        combined = {
            "metadata": {
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
                "contracts": self.contracts,
                "questions": self.question_ids,
                "conditions": self.conditions,
                "total_results": len(results),
            },
            "metrics": metrics,
            "clash_type_metrics": clash_metrics,
            "ab_comparison": ab_comparison,
            "results": results,
        }
        json_path = self.output_dir / "evaluation_results.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(combined, f, indent=2, ensure_ascii=False)
        print(f"[OK] Saved {json_path}")

        # 2. Per-query CSV (with NLP + latency fields)
        per_query_path = self.output_dir / "evaluation_per_query.csv"
        if results:
            fieldnames = [
                "contract_id", "condition", "question_id", "question_target",
                "ground_truth", "expect_clash", "clash_type", "validation_passed",
                "total_attempts", "hard_reject", "hard_reject_reason",
                "accepted_at_attempt", "error",
                "rouge_l", "bertscore_precision", "bertscore_recall", "bertscore_f1",
                "latency_rag", "latency_extraction", "latency_validation", "latency_total",
            ]
            with open(per_query_path, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
                writer.writeheader()
                writer.writerows(results)
            print(f"[OK] Saved {per_query_path}")

        # 3. Per-contract summary CSV
        summary_path = self.output_dir / "evaluation_summary.csv"
        summary_rows = []
        for cid in sorted(set(r["contract_id"] for r in results)):
            for cond in self.conditions:
                rows = [
                    r for r in results
                    if r["contract_id"] == cid and r.get("condition") == cond
                ]
                gt = GROUND_TRUTH.get(cid, {})
                passed = sum(1 for r in rows if r.get("validation_passed") is True)
                failed = sum(1 for r in rows if r.get("validation_passed") is False)
                none_ = sum(1 for r in rows if r.get("validation_passed") is None)
                errors = sum(1 for r in rows if r.get("error"))
                summary_rows.append({
                    "contract_id": cid,
                    "condition": cond,
                    "ground_truth": gt.get("label", "?"),
                    "clash_type": gt.get("clash_type", ""),
                    "queries": len(rows),
                    "passed": passed,
                    "failed": failed,
                    "no_triples": none_,
                    "errors": errors,
                })
        with open(summary_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(
                f,
                fieldnames=["contract_id", "condition", "ground_truth", "clash_type",
                            "queries", "passed", "failed", "no_triples", "errors"],
            )
            writer.writeheader()
            writer.writerows(summary_rows)
        print(f"[OK] Saved {summary_path}")

        # 4. Aggregate metrics CSV (single row)
        metrics_path = self.output_dir / "evaluation_metrics.csv"
        with open(metrics_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=list(metrics.keys()))
            writer.writeheader()
            writer.writerow(metrics)
        print(f"[OK] Saved {metrics_path}")

        # 5. Per clash-type CSV
        clash_path = self.output_dir / "evaluation_per_clash_type.csv"
        clash_rows = []
        for ctype in sorted(clash_metrics.keys()):
            m = clash_metrics[ctype]
            clash_rows.append({
                "clash_type": ctype,
                "tp": m["tp"],
                "fp": m["fp"],
                "tn": m["tn"],
                "fn": m["fn"],
                "total": m["total"],
                "precision": m["precision"],
                "recall": m["recall"],
                "f1": m["f1"],
                "detection_rate": m["detection_rate"],
            })
        with open(clash_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(
                f,
                fieldnames=["clash_type", "tp", "fp", "tn", "fn", "total",
                            "precision", "recall", "f1", "detection_rate"],
            )
            writer.writeheader()
            writer.writerows(clash_rows)
        print(f"[OK] Saved {clash_path}")

        # 6. A/B Comparison CSV
        ab_path = self.output_dir / "evaluation_ab_comparison.csv"
        with open(ab_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(
                f, fieldnames=["metric", "plain_rag", "ovrag", "difference"],
            )
            writer.writeheader()
            writer.writerows(ab_comparison)
        print(f"[OK] Saved {ab_path}")


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(
        description="OV-RAG Evaluation Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python evaluate.py                             # Full run (1000 queries)
  python evaluate.py --dry-run                   # Print plan only
  python evaluate.py --contracts 001 061         # Two contracts
  python evaluate.py --questions Q1 Q3           # Two questions
  python evaluate.py --conditions ovrag          # OV-RAG only
  python evaluate.py --resume                    # Skip completed queries
        """,
    )
    parser.add_argument(
        "--contracts", nargs="+", default=None,
        help="Contract IDs to evaluate (e.g. 001 061). Default: all.",
    )
    parser.add_argument(
        "--questions", nargs="+", default=None,
        help="Question IDs to use (e.g. Q1 Q3). Default: all.",
    )
    parser.add_argument(
        "--conditions", nargs="+", default=None,
        choices=["ovrag", "plain"],
        help="Conditions to run. Default: both.",
    )
    parser.add_argument(
        "--output-dir", default="evaluation_output",
        help="Output directory (default: evaluation_output).",
    )
    parser.add_argument(
        "--resume", action="store_true",
        help="Skip queries already in the JSONL file.",
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="Print the plan without making API calls.",
    )

    args = parser.parse_args()

    # Defaults
    contracts = args.contracts or sorted(GROUND_TRUTH.keys())
    question_ids = args.questions or [q["id"] for q in QUESTIONS]
    conditions = args.conditions or ["ovrag", "plain"]

    # Validate
    for c in contracts:
        if c not in GROUND_TRUTH:
            print(f"[X] Unknown contract ID: {c}")
            print(f"    Valid: {', '.join(sorted(GROUND_TRUTH.keys()))}")
            sys.exit(1)
    for qid in question_ids:
        if qid not in QUESTION_MAP:
            print(f"[X] Unknown question ID: {qid}")
            print(f"    Valid: {', '.join(QUESTION_MAP.keys())}")
            sys.exit(1)

    # Check prerequisites (unless dry-run)
    if not args.dry_run:
        if not os.getenv("OPENAI_API_KEY"):
            print("[X] OPENAI_API_KEY not set")
            sys.exit(1)
        for c in contracts:
            pdf = _contract_pdf(c)
            if not Path(pdf).exists():
                print(f"[X] Missing PDF: {pdf}")
                sys.exit(1)

    runner = EvaluationRunner(
        contracts=contracts,
        questions=question_ids,
        conditions=conditions,
        output_dir=args.output_dir,
        resume=args.resume,
        dry_run=args.dry_run,
    )
    runner.run()


if __name__ == "__main__":
    main()
